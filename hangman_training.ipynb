{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T06:46:00.898434Z",
     "iopub.status.busy": "2025-10-21T06:46:00.897909Z",
     "iopub.status.idle": "2025-10-21T09:22:44.010385Z",
     "shell.execute_reply": "2025-10-21T09:22:44.009540Z",
     "shell.execute_reply.started": "2025-10-21T06:46:00.898411Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data.\n",
      "Data loaded: 205301 train, 19999 val, 2000 test.\n",
      "Training mode enabled.\n",
      "Using device: cuda\n",
      "Starting training process with enhanced gate.\n",
      "Initializing datasets with 52-dim state simulation.\n",
      "Datasets initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/120 [Train]: 100%|██████████| 1604/1604 [01:08<00:00, 23.28it/s]\n",
      "Epoch 1/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120 -> Train Loss: 1.3511, Val Loss: 1.2760, LR: 0.001000\n",
      "  -> Validation loss improved to 1.2760. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.37it/s]\n",
      "Epoch 2/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/120 -> Train Loss: 1.2624, Val Loss: 1.2443, LR: 0.000999\n",
      "  -> Validation loss improved to 1.2443. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.52it/s]\n",
      "Epoch 3/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/120 -> Train Loss: 1.2320, Val Loss: 1.2279, LR: 0.000998\n",
      "  -> Validation loss improved to 1.2279. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.62it/s]\n",
      "Epoch 4/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/120 -> Train Loss: 1.2201, Val Loss: 1.2210, LR: 0.000997\n",
      "  -> Validation loss improved to 1.2210. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.47it/s]\n",
      "Epoch 5/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/120 -> Train Loss: 1.2151, Val Loss: 1.2211, LR: 0.000996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.59it/s]\n",
      "Epoch 6/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/120 -> Train Loss: 1.2098, Val Loss: 1.2013, LR: 0.000994\n",
      "  -> Validation loss improved to 1.2013. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.58it/s]\n",
      "Epoch 7/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/120 -> Train Loss: 1.2008, Val Loss: 1.2023, LR: 0.000992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.46it/s]\n",
      "Epoch 8/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/120 -> Train Loss: 1.1941, Val Loss: 1.1943, LR: 0.000989\n",
      "  -> Validation loss improved to 1.1943. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.35it/s]\n",
      "Epoch 9/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/120 -> Train Loss: 1.1908, Val Loss: 1.1890, LR: 0.000986\n",
      "  -> Validation loss improved to 1.1890. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.29it/s]\n",
      "Epoch 10/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/120 -> Train Loss: 1.1880, Val Loss: 1.1920, LR: 0.000983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.47it/s]\n",
      "Epoch 11/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/120 -> Train Loss: 1.1855, Val Loss: 1.1824, LR: 0.000979\n",
      "  -> Validation loss improved to 1.1824. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.51it/s]\n",
      "Epoch 12/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/120 -> Train Loss: 1.1806, Val Loss: 1.1800, LR: 0.000976\n",
      "  -> Validation loss improved to 1.1800. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.40it/s]\n",
      "Epoch 13/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/120 -> Train Loss: 1.1798, Val Loss: 1.1760, LR: 0.000971\n",
      "  -> Validation loss improved to 1.1760. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.37it/s]\n",
      "Epoch 14/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/120 -> Train Loss: 1.1777, Val Loss: 1.1700, LR: 0.000967\n",
      "  -> Validation loss improved to 1.1700. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.38it/s]\n",
      "Epoch 15/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/120 -> Train Loss: 1.1776, Val Loss: 1.1813, LR: 0.000962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.45it/s]\n",
      "Epoch 16/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/120 -> Train Loss: 1.1727, Val Loss: 1.1705, LR: 0.000957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.44it/s]\n",
      "Epoch 17/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/120 -> Train Loss: 1.1702, Val Loss: 1.1694, LR: 0.000951\n",
      "  -> Validation loss improved to 1.1694. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.36it/s]\n",
      "Epoch 18/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/120 -> Train Loss: 1.1729, Val Loss: 1.1690, LR: 0.000946\n",
      "  -> Validation loss improved to 1.1690. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.45it/s]\n",
      "Epoch 19/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/120 -> Train Loss: 1.1684, Val Loss: 1.1573, LR: 0.000939\n",
      "  -> Validation loss improved to 1.1573. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 20/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/120 -> Train Loss: 1.1662, Val Loss: 1.1721, LR: 0.000933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.43it/s]\n",
      "Epoch 21/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/120 -> Train Loss: 1.1633, Val Loss: 1.1567, LR: 0.000926\n",
      "  -> Validation loss improved to 1.1567. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 22/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/120 -> Train Loss: 1.1646, Val Loss: 1.1639, LR: 0.000919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.43it/s]\n",
      "Epoch 23/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120 -> Train Loss: 1.1636, Val Loss: 1.1666, LR: 0.000912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 24/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/120 -> Train Loss: 1.1619, Val Loss: 1.1628, LR: 0.000905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.40it/s]\n",
      "Epoch 25/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/120 -> Train Loss: 1.1615, Val Loss: 1.1559, LR: 0.000897\n",
      "  -> Validation loss improved to 1.1559. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 26/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/120 -> Train Loss: 1.1564, Val Loss: 1.1668, LR: 0.000889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.45it/s]\n",
      "Epoch 27/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/120 -> Train Loss: 1.1557, Val Loss: 1.1591, LR: 0.000880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.47it/s]\n",
      "Epoch 28/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/120 -> Train Loss: 1.1561, Val Loss: 1.1549, LR: 0.000872\n",
      "  -> Validation loss improved to 1.1549. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.49it/s]\n",
      "Epoch 29/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/120 -> Train Loss: 1.1552, Val Loss: 1.1515, LR: 0.000863\n",
      "  -> Validation loss improved to 1.1515. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.49it/s]\n",
      "Epoch 30/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/120 -> Train Loss: 1.1521, Val Loss: 1.1555, LR: 0.000854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.47it/s]\n",
      "Epoch 31/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/120 -> Train Loss: 1.1488, Val Loss: 1.1437, LR: 0.000844\n",
      "  -> Validation loss improved to 1.1437. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.51it/s]\n",
      "Epoch 32/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/120 -> Train Loss: 1.1501, Val Loss: 1.1557, LR: 0.000835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 33/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/120 -> Train Loss: 1.1487, Val Loss: 1.1591, LR: 0.000825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.46it/s]\n",
      "Epoch 34/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/120 -> Train Loss: 1.1508, Val Loss: 1.1556, LR: 0.000815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 35/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/120 -> Train Loss: 1.1464, Val Loss: 1.1543, LR: 0.000804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.43it/s]\n",
      "Epoch 36/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/120 -> Train Loss: 1.1438, Val Loss: 1.1581, LR: 0.000794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.47it/s]\n",
      "Epoch 37/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/120 -> Train Loss: 1.1451, Val Loss: 1.1523, LR: 0.000783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 38/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/120 -> Train Loss: 1.1475, Val Loss: 1.1473, LR: 0.000772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.43it/s]\n",
      "Epoch 39/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/120 -> Train Loss: 1.1417, Val Loss: 1.1358, LR: 0.000761\n",
      "  -> Validation loss improved to 1.1358. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.44it/s]\n",
      "Epoch 40/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/120 -> Train Loss: 1.1456, Val Loss: 1.1455, LR: 0.000750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 41/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/120 -> Train Loss: 1.1402, Val Loss: 1.1410, LR: 0.000739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.51it/s]\n",
      "Epoch 42/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/120 -> Train Loss: 1.1421, Val Loss: 1.1489, LR: 0.000727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 43/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/120 -> Train Loss: 1.1406, Val Loss: 1.1537, LR: 0.000715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.47it/s]\n",
      "Epoch 44/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/120 -> Train Loss: 1.1359, Val Loss: 1.1519, LR: 0.000703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.45it/s]\n",
      "Epoch 45/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/120 -> Train Loss: 1.1352, Val Loss: 1.1405, LR: 0.000691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.49it/s]\n",
      "Epoch 46/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/120 -> Train Loss: 1.1426, Val Loss: 1.1326, LR: 0.000679\n",
      "  -> Validation loss improved to 1.1326. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.49it/s]\n",
      "Epoch 47/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120 -> Train Loss: 1.1387, Val Loss: 1.1357, LR: 0.000667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.50it/s]\n",
      "Epoch 48/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/120 -> Train Loss: 1.1339, Val Loss: 1.1466, LR: 0.000655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.43it/s]\n",
      "Epoch 49/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/120 -> Train Loss: 1.1367, Val Loss: 1.1431, LR: 0.000642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.38it/s]\n",
      "Epoch 50/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/120 -> Train Loss: 1.1306, Val Loss: 1.1384, LR: 0.000629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.40it/s]\n",
      "Epoch 51/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/120 -> Train Loss: 1.1326, Val Loss: 1.1345, LR: 0.000617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.52it/s]\n",
      "Epoch 52/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/120 -> Train Loss: 1.1326, Val Loss: 1.1446, LR: 0.000604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.60it/s]\n",
      "Epoch 53/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/120 -> Train Loss: 1.1330, Val Loss: 1.1438, LR: 0.000591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.56it/s]\n",
      "Epoch 54/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/120 -> Train Loss: 1.1293, Val Loss: 1.1229, LR: 0.000578\n",
      "  -> Validation loss improved to 1.1229. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.44it/s]\n",
      "Epoch 55/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/120 -> Train Loss: 1.1281, Val Loss: 1.1472, LR: 0.000565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.37it/s]\n",
      "Epoch 56/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/120 -> Train Loss: 1.1278, Val Loss: 1.1373, LR: 0.000552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.56it/s]\n",
      "Epoch 57/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/120 -> Train Loss: 1.1290, Val Loss: 1.1305, LR: 0.000539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.55it/s]\n",
      "Epoch 58/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/120 -> Train Loss: 1.1274, Val Loss: 1.1319, LR: 0.000526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.43it/s]\n",
      "Epoch 59/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/120 -> Train Loss: 1.1236, Val Loss: 1.1371, LR: 0.000513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.35it/s]\n",
      "Epoch 60/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/120 -> Train Loss: 1.1281, Val Loss: 1.1437, LR: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.50it/s]\n",
      "Epoch 61/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/120 -> Train Loss: 1.1172, Val Loss: 1.1321, LR: 0.000487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.51it/s]\n",
      "Epoch 62/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/120 -> Train Loss: 1.1250, Val Loss: 1.1327, LR: 0.000474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.64it/s]\n",
      "Epoch 63/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/120 -> Train Loss: 1.1236, Val Loss: 1.1356, LR: 0.000461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.60it/s]\n",
      "Epoch 64/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/120 -> Train Loss: 1.1201, Val Loss: 1.1309, LR: 0.000448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.58it/s]\n",
      "Epoch 65/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/120 -> Train Loss: 1.1244, Val Loss: 1.1386, LR: 0.000435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 66/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/120 -> Train Loss: 1.1199, Val Loss: 1.1243, LR: 0.000422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.46it/s]\n",
      "Epoch 67/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/120 -> Train Loss: 1.1196, Val Loss: 1.1344, LR: 0.000409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.32it/s]\n",
      "Epoch 68/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/120 -> Train Loss: 1.1177, Val Loss: 1.1340, LR: 0.000396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.38it/s]\n",
      "Epoch 69/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/120 -> Train Loss: 1.1153, Val Loss: 1.1286, LR: 0.000383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.51it/s]\n",
      "Epoch 70/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/120 -> Train Loss: 1.1137, Val Loss: 1.1263, LR: 0.000371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.60it/s]\n",
      "Epoch 71/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/120 -> Train Loss: 1.1184, Val Loss: 1.1167, LR: 0.000358\n",
      "  -> Validation loss improved to 1.1167. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 72/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120 -> Train Loss: 1.1147, Val Loss: 1.1339, LR: 0.000345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.53it/s]\n",
      "Epoch 73/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/120 -> Train Loss: 1.1181, Val Loss: 1.1349, LR: 0.000333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.43it/s]\n",
      "Epoch 74/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/120 -> Train Loss: 1.1123, Val Loss: 1.1363, LR: 0.000321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.36it/s]\n",
      "Epoch 75/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/120 -> Train Loss: 1.1143, Val Loss: 1.1161, LR: 0.000309\n",
      "  -> Validation loss improved to 1.1161. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.36it/s]\n",
      "Epoch 76/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/120 -> Train Loss: 1.1140, Val Loss: 1.1203, LR: 0.000297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.38it/s]\n",
      "Epoch 77/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/120 -> Train Loss: 1.1117, Val Loss: 1.1232, LR: 0.000285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.52it/s]\n",
      "Epoch 78/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/120 -> Train Loss: 1.1110, Val Loss: 1.1279, LR: 0.000273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.61it/s]\n",
      "Epoch 79/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/120 -> Train Loss: 1.1077, Val Loss: 1.1228, LR: 0.000261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 80/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/120 -> Train Loss: 1.1103, Val Loss: 1.1252, LR: 0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.62it/s]\n",
      "Epoch 81/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/120 -> Train Loss: 1.1089, Val Loss: 1.1165, LR: 0.000239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.61it/s]\n",
      "Epoch 82/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/120 -> Train Loss: 1.1087, Val Loss: 1.1223, LR: 0.000228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 83/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/120 -> Train Loss: 1.1064, Val Loss: 1.1069, LR: 0.000217\n",
      "  -> Validation loss improved to 1.1069. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.37it/s]\n",
      "Epoch 84/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/120 -> Train Loss: 1.1074, Val Loss: 1.1251, LR: 0.000206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.36it/s]\n",
      "Epoch 85/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/120 -> Train Loss: 1.1020, Val Loss: 1.1343, LR: 0.000196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.38it/s]\n",
      "Epoch 86/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/120 -> Train Loss: 1.1041, Val Loss: 1.1235, LR: 0.000185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.49it/s]\n",
      "Epoch 87/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/120 -> Train Loss: 1.1050, Val Loss: 1.1095, LR: 0.000175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 88/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/120 -> Train Loss: 1.1039, Val Loss: 1.1217, LR: 0.000165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 89/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/120 -> Train Loss: 1.1057, Val Loss: 1.1193, LR: 0.000156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.48it/s]\n",
      "Epoch 90/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/120 -> Train Loss: 1.1024, Val Loss: 1.1222, LR: 0.000146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.38it/s]\n",
      "Epoch 91/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/120 -> Train Loss: 1.0985, Val Loss: 1.1167, LR: 0.000137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.39it/s]\n",
      "Epoch 92/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/120 -> Train Loss: 1.1021, Val Loss: 1.1340, LR: 0.000128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.50it/s]\n",
      "Epoch 93/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/120 -> Train Loss: 1.1027, Val Loss: 1.1223, LR: 0.000120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.51it/s]\n",
      "Epoch 94/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/120 -> Train Loss: 1.1028, Val Loss: 1.1084, LR: 0.000111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 95/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/120 -> Train Loss: 1.1013, Val Loss: 1.1145, LR: 0.000103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 96/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/120 -> Train Loss: 1.0993, Val Loss: 1.1194, LR: 0.000095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 97/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 53.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/120 -> Train Loss: 1.1003, Val Loss: 1.1275, LR: 0.000088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.14it/s]\n",
      "Epoch 98/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 54.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/120 -> Train Loss: 1.0977, Val Loss: 1.1152, LR: 0.000081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.17it/s]\n",
      "Epoch 99/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 53.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/120 -> Train Loss: 1.0988, Val Loss: 1.1162, LR: 0.000074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.18it/s]\n",
      "Epoch 100/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 53.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/120 -> Train Loss: 1.0959, Val Loss: 1.1293, LR: 0.000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/120 [Train]: 100%|██████████| 1604/1604 [01:16<00:00, 21.08it/s]\n",
      "Epoch 101/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 53.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/120 -> Train Loss: 1.0972, Val Loss: 1.1216, LR: 0.000061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.15it/s]\n",
      "Epoch 102/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 54.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/120 -> Train Loss: 1.0990, Val Loss: 1.1079, LR: 0.000054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.17it/s]\n",
      "Epoch 103/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/120 -> Train Loss: 1.1003, Val Loss: 1.0969, LR: 0.000049\n",
      "  -> Validation loss improved to 1.0969. Saving model to bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 104/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 54.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/120 -> Train Loss: 1.0983, Val Loss: 1.1203, LR: 0.000043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.37it/s]\n",
      "Epoch 105/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/120 -> Train Loss: 1.0947, Val Loss: 1.1119, LR: 0.000038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.45it/s]\n",
      "Epoch 106/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/120 -> Train Loss: 1.0998, Val Loss: 1.1093, LR: 0.000033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.65it/s]\n",
      "Epoch 107/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/120 -> Train Loss: 1.0965, Val Loss: 1.1256, LR: 0.000029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.56it/s]\n",
      "Epoch 108/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/120 -> Train Loss: 1.0936, Val Loss: 1.1070, LR: 0.000024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.42it/s]\n",
      "Epoch 109/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/120 -> Train Loss: 1.0962, Val Loss: 1.1098, LR: 0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.46it/s]\n",
      "Epoch 110/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/120 -> Train Loss: 1.0931, Val Loss: 1.0992, LR: 0.000017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.55it/s]\n",
      "Epoch 111/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/120 -> Train Loss: 1.0934, Val Loss: 1.1246, LR: 0.000014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.58it/s]\n",
      "Epoch 112/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/120 -> Train Loss: 1.0967, Val Loss: 1.1162, LR: 0.000011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.59it/s]\n",
      "Epoch 113/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/120 -> Train Loss: 1.0945, Val Loss: 1.1124, LR: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.56it/s]\n",
      "Epoch 114/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/120 -> Train Loss: 1.0952, Val Loss: 1.1140, LR: 0.000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 115/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/120 -> Train Loss: 1.0952, Val Loss: 1.1191, LR: 0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 116/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/120 -> Train Loss: 1.0948, Val Loss: 1.1074, LR: 0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.57it/s]\n",
      "Epoch 117/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/120 -> Train Loss: 1.0906, Val Loss: 1.1213, LR: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.58it/s]\n",
      "Epoch 118/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 56.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120 -> Train Loss: 1.0952, Val Loss: 1.1152, LR: 0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/120 [Train]: 100%|██████████| 1604/1604 [01:14<00:00, 21.44it/s]\n",
      "Epoch 119/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/120 -> Train Loss: 1.0943, Val Loss: 1.1125, LR: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/120 [Train]: 100%|██████████| 1604/1604 [01:15<00:00, 21.36it/s]\n",
      "Epoch 120/120 [Val]: 100%|██████████| 157/157 [00:02<00:00, 55.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120 -> Train Loss: 1.0943, Val Loss: 1.1075, LR: 0.000000\n",
      "Training complete. Loading best model from bilstm_attn_hangman_GATED_MLPGATE_improv2.pth for evaluation.\n",
      "\n",
      "--- STARTING FINAL EVALUATION (MAX CAPACITY GATED-ATTN) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Final Agent: 100%|██████████| 2000/2000 [01:32<00:00, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EVALUATION COMPLETE ---\n",
      "Final Agent won 1240 out of 2000 games.\n",
      "Final Accuracy: 62.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "\n",
    "# configs\n",
    "EXPERT_TRAIN_DICT_PATH = \"/kaggle/input/hman-ds/expert_train.txt\"\n",
    "FINAL_TEST_DICT_PATH = \"/kaggle/input/hman-ds/final_test.txt\"\n",
    "RL_VAL_DICT_PATH = \"/kaggle/input/hman-ds/rl_train.txt\"\n",
    "MODEL_FILE = \"bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "D_MODEL = 128       \n",
    "HIDDEN_DIM = 256    \n",
    "NUM_LAYERS = 4      \n",
    "ATTN_HEADS = 8      \n",
    "MAX_SEQ_LEN = 32\n",
    "ALPHABET_LEN = 26   \n",
    "STATE_DIM = ALPHABET_LEN * 2 \n",
    "MAX_SIMULATED_WRONG_GUESSES = 6 \n",
    "\n",
    "EPOCHS = 120\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "WEIGHT_DECAY = 1e-6 \n",
    "\n",
    "CHAR_TO_IX = {char: i + 2 for i, char in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n",
    "CHAR_TO_IX['_'] = 1\n",
    "VOCAB_SIZE = len(CHAR_TO_IX) + 1\n",
    "IX_TO_CHAR = {i: char for char, i in CHAR_TO_IX.items()}\n",
    "IX_TO_CHAR[0] = '<pad>'\n",
    "ALL_LETTERS = set(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "\n",
    "# arch\n",
    "class Gated_BiLSTM_Attention_Hangman(nn.Module):\n",
    "    \"\"\"\n",
    "    BiLSTM with Self-Attention and Gated Fusion using a MLP Gate.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, state_dim, num_heads):\n",
    "        super(Gated_BiLSTM_Attention_Hangman, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm_output_dim = hidden_dim * 2  # 512\n",
    "        \n",
    "        # BiLSTM\n",
    "        lstm_input_dim = embedding_dim\n",
    "        self.lstm = nn.LSTM(\n",
    "            lstm_input_dim, hidden_dim, num_layers=num_layers,\n",
    "            bidirectional=True, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # self-Attention layer\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=self.lstm_output_dim, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(self.lstm_output_dim)\n",
    "\n",
    "        # deep projection for Game State (C_State)\n",
    "        self.state_projection = nn.Sequential(\n",
    "            nn.Linear(state_dim, self.lstm_output_dim),                                     # initial projection to match D_out\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.lstm_output_dim, self.lstm_output_dim)\n",
    "        )\n",
    "\n",
    "        # the MLP gate\n",
    "        # Input size is [O_Attn; C_State] = 2 * lstm_output_dim = 1024D\n",
    "        self.gate_fc = nn.Sequential(\n",
    "            nn.Linear(self.lstm_output_dim * 2, self.lstm_output_dim),                      # input: 1024D, Output: 512D\n",
    "            nn.ReLU(),                                                                      # added non-linearity.\n",
    "            nn.Linear(self.lstm_output_dim, self.lstm_output_dim),                          # input: 512D, Output: 512D (Gate Vector)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # final classificatier\n",
    "        self.fc = nn.Linear(self.lstm_output_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        \n",
    "        attn_key_padding_mask = (x == 0) \n",
    "        embedded = self.embedding(x) \n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embedded) \n",
    "\n",
    "        attn_output, _ = self.attention(\n",
    "            lstm_out, lstm_out, lstm_out, key_padding_mask=attn_key_padding_mask\n",
    "        )\n",
    "        O_Attn = self.norm(lstm_out + attn_output) \n",
    "        \n",
    "        C_State_projected = self.state_projection(state) \n",
    "        C_State = C_State_projected.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        combined_features = torch.cat([O_Attn, C_State], dim=-1)\n",
    "        # Use the deep MLP gate\n",
    "        Gamma = self.gate_fc(combined_features) \n",
    "        \n",
    "        O_Gated = Gamma * O_Attn + (1 - Gamma) * C_State\n",
    "        \n",
    "        return self.fc(O_Gated)\n",
    "\n",
    "\n",
    "class HangmanDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Generates the 52D (deep) state vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_list, max_seq_len, max_simulated_wrong_guesses, name=\"\"):\n",
    "        self.max_simulated_wrong_guesses = max_simulated_wrong_guesses\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        original_count = len(word_list)\n",
    "        self.word_list = [word for word in word_list if word and len(word) < self.max_seq_len]\n",
    "        filtered_count = original_count - len(self.word_list)\n",
    "        if filtered_count > 0:\n",
    "            print(f\"  -> [{name} dataset] Filtered {filtered_count} words (empty or > {self.max_seq_len} chars).\")\n",
    "\n",
    "    def __len__(self): return len(self.word_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.word_list[idx]\n",
    "        secret_letters = set(word)\n",
    "        \n",
    "        mask_count = random.randint(1, len(word))\n",
    "        mask_indices = sorted(random.sample(range(len(word)), k=mask_count))\n",
    "        masked_word_list = list(word)\n",
    "        for i in mask_indices: masked_word_list[i] = '_'\n",
    "        masked_word = \"\".join(masked_word_list)\n",
    "        input_seq = [CHAR_TO_IX[c] for c in masked_word]\n",
    "        target_seq = [CHAR_TO_IX[c] for c in word]\n",
    "\n",
    "        visible_letters = set(c for c in masked_word if c != '_')\n",
    "        incorrect_letters_pool = list(ALL_LETTERS - secret_letters)\n",
    "        \n",
    "        num_incorrect_to_simulate = random.randint(0, self.max_simulated_wrong_guesses)\n",
    "        num_incorrect_to_simulate = min(num_incorrect_to_simulate, len(incorrect_letters_pool))\n",
    "        \n",
    "        simulated_incorrect_guesses = set(random.sample(incorrect_letters_pool, k=num_incorrect_to_simulate))\n",
    "        \n",
    "        state_vector = torch.zeros(ALPHABET_LEN * 2) \n",
    "\n",
    "        for char in visible_letters:\n",
    "            if char in CHAR_TO_IX:\n",
    "                char_idx = CHAR_TO_IX[char] - 2 \n",
    "                if 0 <= char_idx < ALPHABET_LEN:\n",
    "                    state_vector[char_idx] = 1.0\n",
    "\n",
    "        for char in simulated_incorrect_guesses:\n",
    "            if char in CHAR_TO_IX:\n",
    "                char_idx = CHAR_TO_IX[char] - 2 \n",
    "                if 0 <= char_idx < ALPHABET_LEN:\n",
    "                    state_vector[char_idx + ALPHABET_LEN] = 1.0\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_seq, dtype=torch.long),\n",
    "            torch.tensor(target_seq, dtype=torch.long),\n",
    "            state_vector\n",
    "        )\n",
    "\n",
    "# solver trainer\n",
    "class IntelligentHangmanSolver:\n",
    "    def __init__(self, full_train_dict, train_mode=False, train_words=None, val_words=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.all_letters = ALL_LETTERS \n",
    "        self.all_letters_list = \"abcdefghijklmnopqrstuvwxyz\" \n",
    "        \n",
    "        self._load_or_train_model(train_mode, train_words, val_words)\n",
    "\n",
    "    def _load_or_train_model(self, train_mode, train_words, val_words):\n",
    "        # instantiates the model\n",
    "        self.model = Gated_BiLSTM_Attention_Hangman(\n",
    "            VOCAB_SIZE, D_MODEL, HIDDEN_DIM, NUM_LAYERS, state_dim=STATE_DIM, num_heads=ATTN_HEADS\n",
    "        ).to(self.device)\n",
    "\n",
    "        if train_mode and train_words and val_words:\n",
    "            print(\"Starting training process with enhanced gate.\")\n",
    "            self._train_model(train_words, val_words) \n",
    "            print(f\"Training complete. Loading best model from {MODEL_FILE} for evaluation.\")\n",
    "            \n",
    "            if os.path.exists(MODEL_FILE):\n",
    "                self.model.load_state_dict(torch.load(MODEL_FILE, map_location=self.device))\n",
    "            else:\n",
    "                print(\"Warning: Best model was not saved.\")\n",
    "        \n",
    "        elif os.path.exists(MODEL_FILE):\n",
    "            print(f\"Loading pre-trained model from {MODEL_FILE}.\")\n",
    "            self.model.load_state_dict(torch.load(MODEL_FILE, map_location=self.device))\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file '{MODEL_FILE}' not found and not in train mode. Please enable training.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "    def _train_model(self, train_words, val_words):\n",
    "        print(\"Initializing datasets with 52-dim state simulation.\")\n",
    "        \n",
    "        train_dataset = HangmanDataset(train_words, MAX_SEQ_LEN, MAX_SIMULATED_WRONG_GUESSES, name=\"Train\")\n",
    "        val_dataset = HangmanDataset(val_words, MAX_SEQ_LEN, MAX_SIMULATED_WRONG_GUESSES, name=\"Validation\")\n",
    "        \n",
    "        print(\"Datasets initialized.\")\n",
    "\n",
    "        def collate_fn(batch):\n",
    "            inputs, targets, states = zip(*batch)\n",
    "            inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "            targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=0)\n",
    "            states_batched = torch.stack(states, dim=0)\n",
    "            return inputs_padded, targets_padded, states_batched\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            self.model.train()\n",
    "            total_train_loss = 0\n",
    "            for inputs, targets, states in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "                inputs, targets, states = inputs.to(self.device), targets.to(self.device), states.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs, states) \n",
    "                loss = criterion(outputs.view(-1, VOCAB_SIZE), targets.view(-1))\n",
    "                if math.isnan(loss.item()): print(\"\\n!!! Train NaN !!!\"); return\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_train_loss += loss.item()\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "            self.model.eval()\n",
    "            total_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, targets, states in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "                    inputs, targets, states = inputs.to(self.device), targets.to(self.device), states.to(self.device)\n",
    "                    outputs = self.model(inputs, states) \n",
    "                    loss = criterion(outputs.view(-1, VOCAB_SIZE), targets.view(-1))\n",
    "                    if math.isnan(loss.item()): print(\"\\n!!! Val NaN !!!\"); return\n",
    "                    total_val_loss += loss.item()\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "            scheduler.step()\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} -> Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(self.model.state_dict(), MODEL_FILE)\n",
    "                print(f\"  -> Validation loss improved to {best_val_loss:.4f}. Saving model to {MODEL_FILE}\")\n",
    "\n",
    "    # inference fn, creates 52-D state\n",
    "    def _get_lstm_scores(self, pattern, guessed_letters_set):\n",
    "        self.model.eval()\n",
    "        input_seq = torch.tensor([[CHAR_TO_IX.get(c, 0) for c in pattern]], dtype=torch.long).to(self.device)\n",
    "        \n",
    "        visible_letters = set(c for c in pattern if c.isalpha())\n",
    "        incorrect_guesses = guessed_letters_set - visible_letters\n",
    "        \n",
    "        state_vector = torch.zeros(ALPHABET_LEN * 2) \n",
    "\n",
    "        for char in visible_letters:\n",
    "            if char in CHAR_TO_IX:\n",
    "                char_idx = CHAR_TO_IX[char] - 2\n",
    "                if 0 <= char_idx < ALPHABET_LEN:\n",
    "                    state_vector[char_idx] = 1.0\n",
    "        \n",
    "        for char in incorrect_guesses:\n",
    "            if char in CHAR_TO_IX:\n",
    "                char_idx = CHAR_TO_IX[char] - 2\n",
    "                if 0 <= char_idx < ALPHABET_LEN:\n",
    "                    state_vector[char_idx + ALPHABET_LEN] = 1.0\n",
    "        \n",
    "        state_tensor = state_vector.unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_seq, state_tensor) \n",
    "            probabilities = torch.softmax(logits, dim=2).squeeze(0)\n",
    "            \n",
    "        letter_scores = {}\n",
    "        for i, char in enumerate(pattern):\n",
    "            if char == '_':\n",
    "                for letter_idx, letter in IX_TO_CHAR.items():\n",
    "                    if letter_idx >= 2: \n",
    "                        prob = probabilities[i, letter_idx].item()\n",
    "                        letter_scores[letter] = max(letter_scores.get(letter, 0.0), prob)\n",
    "        return letter_scores\n",
    "\n",
    "    def choose_letter(self, pattern, guessed_letters_set):\n",
    "        lstm_scores = self._get_lstm_scores(pattern, guessed_letters_set) \n",
    "        all_possible_letters = self.all_letters - guessed_letters_set\n",
    "        best_letter = None\n",
    "        max_score = -1.0\n",
    "        \n",
    "        for letter in all_possible_letters:\n",
    "            score = lstm_scores.get(letter, 0.0)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_letter = letter\n",
    "                \n",
    "        if best_letter is not None:\n",
    "            return best_letter\n",
    "        else:\n",
    "            fallback_order = \"esiarntolcdugpmhbyfvwkxqz\"\n",
    "            for letter in fallback_order:\n",
    "                if letter not in guessed_letters_set: return letter\n",
    "            return 'a'\n",
    "\n",
    "class LocalGameSimulator:\n",
    "    def __init__(self, secret_word, max_guesses=6):\n",
    "        self.secret_word = secret_word.lower()\n",
    "        self.max_guesses = max_guesses\n",
    "        self.lives_remaining = self.max_guesses\n",
    "        self.guessed_letters = set()\n",
    "        self.pattern = ['_'] * len(self.secret_word)\n",
    "    def get_pattern(self): return \"\".join(self.pattern)\n",
    "    def is_won(self): return '_' not in self.pattern\n",
    "    def is_lost(self): return self.lives_remaining <= 0\n",
    "    def is_game_over(self): return self.is_won() or self.is_lost()\n",
    "    def guess(self, letter_char):\n",
    "        if letter_char in self.guessed_letters: return\n",
    "        self.guessed_letters.add(letter_char)\n",
    "        if letter_char in self.secret_word:\n",
    "            for i, char in enumerate(self.secret_word):\n",
    "                if char == letter_char: self.pattern[i] = letter_char\n",
    "        else: self.lives_remaining -= 1\n",
    "\n",
    "def run_local_test(solver, test_words):\n",
    "    print(f\"\\n--- STARTING FINAL EVALUATION (MAX CAPACITY GATED-ATTN) ---\")\n",
    "    original_count = len(test_words)\n",
    "    words_to_play = [w for w in test_words if w and len(w) < MAX_SEQ_LEN]\n",
    "    filtered_count = original_count - len(words_to_play)\n",
    "    if filtered_count > 0: print(f\"  -> Filtered {filtered_count} test words.\")\n",
    "    if len(words_to_play) == 0: print(\"Error: No valid test words.\"); return\n",
    "    wins = 0\n",
    "    for secret_word in tqdm(words_to_play, desc=\"Evaluating Final Agent\"):\n",
    "        game = LocalGameSimulator(secret_word)\n",
    "        while not game.is_game_over():\n",
    "            current_pattern = game.get_pattern()\n",
    "            guessed_letter = solver.choose_letter(current_pattern, game.guessed_letters)\n",
    "            game.guess(guessed_letter)\n",
    "        if game.is_won(): wins += 1\n",
    "    accuracy = (wins / len(words_to_play)) * 100\n",
    "    print(\"\\n--- EVALUATION COMPLETE ---\")\n",
    "    print(f\"Final Agent won {wins} out of {len(words_to_play)} games.\")\n",
    "    print(f\"Final Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FORCE_TRAINING = True\n",
    "    if not all(os.path.exists(p) for p in [EXPERT_TRAIN_DICT_PATH, FINAL_TEST_DICT_PATH, RL_VAL_DICT_PATH]):\n",
    "        print(\"ERROR: Ensure data files 'expert_train.txt', 'final_test.txt', and 'rl_train.txt' are present.\")\n",
    "    else:\n",
    "        print(\"Loading data\")\n",
    "        with open(EXPERT_TRAIN_DICT_PATH, \"r\") as f: train_words = [l.strip().lower() for l in f if l.strip()]\n",
    "        with open(RL_VAL_DICT_PATH, \"r\") as f: val_words = [l.strip().lower() for l in f if l.strip()]\n",
    "        with open(FINAL_TEST_DICT_PATH, \"r\") as f: test_words = [l.strip().lower() for l in f if l.strip()]\n",
    "        print(f\"Data loaded: {len(train_words)} train, {len(val_words)} val, {len(test_words)} test.\")\n",
    "        \n",
    "        should_train = FORCE_TRAINING or not os.path.exists(MODEL_FILE)\n",
    "        \n",
    "        if should_train:\n",
    "            print(\"Training mode enabled.\")\n",
    "            solver = IntelligentHangmanSolver(train_words, True, train_words, val_words) \n",
    "        else:\n",
    "            print(f\"Found existing model at {MODEL_FILE}. Skipping training.\")\n",
    "            solver = IntelligentHangmanSolver(train_words, False)\n",
    "            \n",
    "        run_local_test(solver, test_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-21T09:26:20.912956Z",
     "iopub.status.busy": "2025-10-21T09:26:20.912381Z",
     "iopub.status.idle": "2025-10-21T09:34:24.929708Z",
     "shell.execute_reply": "2025-10-21T09:34:24.929014Z",
     "shell.execute_reply.started": "2025-10-21T09:26:20.912932Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dictionary data.\n",
      "Data loaded. Using a dictionary of 225300 words for DAWG calculation.\n",
      "Using device: cuda\n",
      "Loading pre-trained 62% model from bilstm_attn_hangman_GATED_MLPGATE_improv2.pth.\n",
      "\n",
      "--- STARTING FINAL EVALUATION (DAWG Late Fusion) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Final Agent: 100%|██████████| 2000/2000 [08:01<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EVALUATION COMPLETE ---\n",
      "Final Agent won 1263 out of 2000 games.\n",
      "Final Accuracy: 63.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "# configs\n",
    "EXPERT_TRAIN_DICT_PATH = \"/kaggle/input/hman-ds/expert_train.txt\"\n",
    "FINAL_TEST_DICT_PATH = \"/kaggle/input/hman-ds/final_test.txt\"\n",
    "RL_VAL_DICT_PATH = \"/kaggle/input/hman-ds/rl_train.txt\"\n",
    "MODEL_FILE = \"bilstm_attn_hangman_GATED_MLPGATE_improv2.pth\"\n",
    "\n",
    "USE_DAWG_FUSION = True\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "D_MODEL = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 4\n",
    "ATTN_HEADS = 8\n",
    "MAX_SEQ_LEN = 32\n",
    "ALPHABET_LEN = 26\n",
    "STATE_DIM = ALPHABET_LEN * 2\n",
    "\n",
    "CHAR_TO_IX = {char: i + 2 for i, char in enumerate(\"abcdefghijklmnopqrstuvwxyz\")}\n",
    "CHAR_TO_IX['_'] = 1\n",
    "VOCAB_SIZE = len(CHAR_TO_IX) + 1\n",
    "IX_TO_CHAR = {i: char for char, i in CHAR_TO_IX.items()}\n",
    "IX_TO_CHAR[0] = '<pad>'\n",
    "ALL_LETTERS = set(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "\n",
    "# arch\n",
    "class Gated_BiLSTM_Attention_Hangman(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, state_dim, num_heads):\n",
    "        super(Gated_BiLSTM_Attention_Hangman, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm_output_dim = hidden_dim * 2\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.lstm_output_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(self.lstm_output_dim)\n",
    "        self.state_projection = nn.Sequential(\n",
    "            nn.Linear(state_dim, self.lstm_output_dim), nn.ReLU(),\n",
    "            nn.Linear(self.lstm_output_dim, self.lstm_output_dim))\n",
    "        self.gate_fc = nn.Sequential(\n",
    "            nn.Linear(self.lstm_output_dim * 2, self.lstm_output_dim), nn.ReLU(),\n",
    "            nn.Linear(self.lstm_output_dim, self.lstm_output_dim), nn.Sigmoid())\n",
    "        self.fc = nn.Linear(self.lstm_output_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        attn_key_padding_mask, embedded, seq_len = (x == 0), self.embedding(x), x.size(1)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out, key_padding_mask=attn_key_padding_mask)\n",
    "        O_Attn = self.norm(lstm_out + attn_output)\n",
    "        C_State_projected = self.state_projection(state)\n",
    "        C_State = C_State_projected.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        combined_features = torch.cat([O_Attn, C_State], dim=-1)\n",
    "        Gamma = self.gate_fc(combined_features)\n",
    "        O_Gated = Gamma * O_Attn + (1 - Gamma) * C_State\n",
    "        return self.fc(O_Gated)\n",
    "\n",
    "class IntelligentHangmanSolver:\n",
    "    def __init__(self, full_word_dictionary):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.all_letters = ALL_LETTERS\n",
    "        self.full_dictionary = full_word_dictionary\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        self.model = Gated_BiLSTM_Attention_Hangman(VOCAB_SIZE, D_MODEL, HIDDEN_DIM, NUM_LAYERS, state_dim=STATE_DIM, num_heads=ATTN_HEADS).to(self.device)\n",
    "        if os.path.exists(MODEL_FILE):\n",
    "            print(f\"Loading pre-trained 62% model from {MODEL_FILE}.\")\n",
    "            self.model.load_state_dict(torch.load(MODEL_FILE, map_location=self.device))\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model file '{MODEL_FILE}' not found.\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def _get_lstm_probs(self, pattern, guessed_letters_set):\n",
    "        self.model.eval()\n",
    "        input_seq = torch.tensor([[CHAR_TO_IX.get(c, 0) for c in pattern]], dtype=torch.long).to(self.device)\n",
    "        visible_letters, incorrect_guesses = {c for c in pattern if c.isalpha()}, guessed_letters_set - {c for c in pattern if c.isalpha()}\n",
    "        state_vector = torch.zeros(ALPHABET_LEN * 2)\n",
    "        for char in visible_letters:\n",
    "            if char in CHAR_TO_IX: state_vector[CHAR_TO_IX[char] - 2] = 1.0\n",
    "        for char in incorrect_guesses:\n",
    "            if char in CHAR_TO_IX: state_vector[CHAR_TO_IX[char] - 2 + ALPHABET_LEN] = 1.0\n",
    "        state_tensor = state_vector.unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_seq, state_tensor)\n",
    "            probabilities = torch.softmax(logits, dim=2).squeeze(0)\n",
    "        \n",
    "        # Aggregate probabilities across all '_' positions\n",
    "        letter_probs = {}\n",
    "        for i, char in enumerate(pattern):\n",
    "            if char == '_':\n",
    "                for letter_idx, letter in IX_TO_CHAR.items():\n",
    "                    if letter_idx >= 2:\n",
    "                        prob = probabilities[i, letter_idx].item()\n",
    "                        letter_probs[letter] = max(letter_probs.get(letter, 0.0), prob)\n",
    "        return letter_probs\n",
    "\n",
    "    def _filter_words_py(self, pattern: str, incorrect_guesses: set) -> list:\n",
    "        valid_words, pattern_len = [], len(pattern)\n",
    "        try: compiled_regex = re.compile(\"^\" + pattern.replace('_', '.') + \"$\")\n",
    "        except re.error: return []\n",
    "        for word in self.full_dictionary:\n",
    "            if len(word) == pattern_len and compiled_regex.match(word) and not any(g in word for g in incorrect_guesses):\n",
    "                valid_words.append(word)\n",
    "        return valid_words\n",
    "\n",
    "    def _get_dawg_probs(self, valid_words: list, guessed_letters: set) -> dict:\n",
    "        freq_counter = Counter(\"\".join(valid_words))\n",
    "        dawg_probs = {}\n",
    "        \n",
    "        # tensor of scores for softmax\n",
    "        scores_tensor = torch.zeros(ALPHABET_LEN)\n",
    "        for i, letter in enumerate(\"abcdefghijklmnopqrstuvwxyz\"):\n",
    "            if letter not in guessed_letters:\n",
    "                scores_tensor[i] = freq_counter.get(letter, 0)\n",
    "\n",
    "        # Normalizes with softmax to get a probability distribution\n",
    "        if torch.sum(scores_tensor) > 0:\n",
    "            probs_tensor = torch.softmax(scores_tensor, dim=0)\n",
    "            for i, letter in enumerate(\"abcdefghijklmnopqrstuvwxyz\"):\n",
    "                dawg_probs[letter] = probs_tensor[i].item()\n",
    "        \n",
    "        return dawg_probs\n",
    "\n",
    "    def choose_letter(self, pattern, guessed_letters_set):\n",
    "        lstm_probs = self._get_lstm_probs(pattern, guessed_letters_set)\n",
    "        \n",
    "        final_scores = lstm_probs\n",
    "        \n",
    "        if USE_DAWG_FUSION:\n",
    "            incorrect_guesses = guessed_letters_set - {c for c in pattern if c.isalpha()}\n",
    "            valid_words = self._filter_words_py(pattern, incorrect_guesses)\n",
    "            dawg_probs = self._get_dawg_probs(valid_words, guessed_letters_set)\n",
    "\n",
    "            # Fuse by simple addition of probabilities\n",
    "            for letter in dawg_probs:\n",
    "                final_scores[letter] = final_scores.get(letter, 0.0) + dawg_probs[letter]\n",
    "\n",
    "        # chooses the best letter based on the combined score ( argmax)\n",
    "        best_letter, max_score = None, -1.0\n",
    "        for letter in self.all_letters - guessed_letters_set:\n",
    "            score = final_scores.get(letter, 0.0)\n",
    "            if score > max_score: max_score, best_letter = score, letter\n",
    "        \n",
    "        if best_letter is not None: return best_letter\n",
    "        for letter in \"esiarntolcdugpmhbyfvwkxqz\":\n",
    "            if letter not in guessed_letters_set: return letter\n",
    "        return 'a'\n",
    "\n",
    "class LocalGameSimulator:\n",
    "    def __init__(self, secret_word, max_guesses=6):\n",
    "        self.secret_word, self.lives_remaining, self.guessed_letters = secret_word.lower(), max_guesses, set()\n",
    "        self.pattern = ['_'] * len(self.secret_word)\n",
    "    def get_pattern(self): return \"\".join(self.pattern)\n",
    "    def is_won(self): return '_' not in self.pattern\n",
    "    def is_lost(self): return self.lives_remaining <= 0\n",
    "    def is_game_over(self): return self.is_won() or self.is_lost()\n",
    "    def guess(self, letter_char):\n",
    "        if letter_char is None or letter_char in self.guessed_letters: return\n",
    "        self.guessed_letters.add(letter_char)\n",
    "        if letter_char in self.secret_word:\n",
    "            for i, char in enumerate(self.secret_word):\n",
    "                if char == letter_char: self.pattern[i] = char\n",
    "        else: self.lives_remaining -= 1\n",
    "\n",
    "def run_local_test(solver, test_words):\n",
    "    fusion_str = \"DAWG Late Fusion\" if USE_DAWG_FUSION else \"LSTM Only\"\n",
    "    print(f\"\\n--- STARTING FINAL EVALUATION ({fusion_str}) ---\")\n",
    "    words_to_play = [w for w in test_words if w and len(w) < MAX_SEQ_LEN]\n",
    "    if not words_to_play: print(\"Error: No valid test words.\"); return\n",
    "    wins = 0\n",
    "    for secret_word in tqdm(words_to_play, desc=\"Evaluating Final Agent\"):\n",
    "        game = LocalGameSimulator(secret_word)\n",
    "        while not game.is_game_over():\n",
    "            game.guess(solver.choose_letter(game.get_pattern(), game.guessed_letters))\n",
    "        if game.is_won(): wins += 1\n",
    "    accuracy = (wins / len(words_to_play)) * 100\n",
    "    print(\"\\n--- EVALUATION COMPLETE ---\")\n",
    "    print(f\"Final Agent won {wins} out of {len(words_to_play)} games.\")\n",
    "    print(f\"Final Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FORCE_TRAINING = False \n",
    "    \n",
    "    if not all(os.path.exists(p) for p in [EXPERT_TRAIN_DICT_PATH, FINAL_TEST_DICT_PATH, RL_VAL_DICT_PATH]):\n",
    "        print(\"ERROR: Ensure data files are present.\")\n",
    "    else:\n",
    "        print(\"Loading dictionary data.\")\n",
    "        with open(EXPERT_TRAIN_DICT_PATH, \"r\") as f: train_words = [l.strip().lower() for l in f if l.strip()]\n",
    "        with open(RL_VAL_DICT_PATH, \"r\") as f: val_words = [l.strip().lower() for l in f if l.strip()]\n",
    "        full_dictionary = list(set(train_words + val_words))\n",
    "        with open(FINAL_TEST_DICT_PATH, \"r\") as f: test_words = [l.strip().lower() for l in f if l.strip()]\n",
    "        print(f\"Data loaded. Using a dictionary of {len(full_dictionary)} words for DAWG calculation.\")\n",
    "        \n",
    "        if not FORCE_TRAINING and os.path.exists(MODEL_FILE):\n",
    "            solver = IntelligentHangmanSolver(full_dictionary)\n",
    "            run_local_test(solver, test_words)\n",
    "        else:\n",
    "            print(f\"Training is disabled or model file '{MODEL_FILE}' not found. Cannot run evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8511150,
     "sourceId": 13410940,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8530427,
     "sourceId": 13439642,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8539097,
     "sourceId": 13452559,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31155,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
